<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Shangqing Tu | publications</title>
  <meta name="description" content="hello">

  <link rel="shortcut icon" href="https://shangqingtu.github.io/assets/img/favicon.ico">

  <link rel="stylesheet" href="https://shangqingtu.github.io/assets/css/main.css">
  <link rel="canonical" href="https://shangqingtu.github.io/publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        Shangqing Tu
    </span>


    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://shangqingtu.github.io/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="https://shangqingtu.github.io/blog/">blog</a> -->

        <!-- News -->
        <a class="page-link" href="https://shangqingtu.github.io/news/">news</a>

        <!-- Pubs -->
        <a class="page-link" href="https://shangqingtu.github.io/publications/">publications</a>

        <!-- Teaching -->
       <a class="page-link" href="https://shangqingtu.github.io/teaching/">teaching</a>


        <!-- Pages -->
<!--







            <a class="page-link" href="https://shangqingtu.github.io/news/">news</a>



            <a class="page-link" href="https://shangqingtu.github.io/publications/">publications</a>



            <a class="page-link" href="https://shangqingtu.github.io/teaching/">teaching</a>





         -->

        <!-- CV link -->
        <a class="page-link" href="https://shangqingtu.github.io/assets/CV__Academia_.pdf">cv</a>

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description">Generated by jekyll-scholar.</h5>
  </header>

  <article class="post-content publications clearfix">
  
    <h3 class="year">Preprints</h3>
    <ol class="bibliography">
      <li>
          <div id="glm4.5-2023">
  
              <span class="title">GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</span>
              <span class="author">
                 GLM-4.5 Team: Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, ...,<em>Shangqing Tu</em>, ... , Jie Tang
              </span>
  
              <span class="periodical">
  
      <em>arxiv-2025-08</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2508.06471">pdf</a>] [<a
                      href="https://github.com/zai-org/GLM-4.5">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.</p>
  </span>
  
          </div>
          </li>
          <li>
          <div id="glm-4.5v-2025">
  
              <span class="title">GLM-4.1V-Thinking and GLM-4.5V: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</span>
              <span class="author">
                GLM-V Team: Wenyi Hong, Wenmeng Yu, Xiaotao Gu, ..., <em>Shangqing Tu</em>, ..., Jie Tang
              </span>
  
              <span class="periodical">
  
      <em>arxiv-2025-07</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/abs/2507.01006">pdf</a>] [<a
                      href="https://github.com/zai-org/GLM-V">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models (VLMs) designed to advance general-purpose multimodal understanding and reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum Sampling (RLCS) to unlock the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document interpretation. In a comprehensive evaluation across 42 public benchmarks, GLM-4.5V achieves state-of-the-art performance on nearly all tasks among open-source models of similar size, and demonstrates competitive or even superior results compared to closed-source models such as Gemini-2.5-Flash on challenging tasks including Coding and GUI Agents. Meanwhile, the smaller GLM-4.1V-9B-Thinking remains highly competitive-achieving superior results to the much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source both GLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information are released at https://github.com/zai-org/GLM-V.</p>
  </span>
  
          </div>
          </li>
        <li>
          <div id="chatlog-2023">
  
              <span class="title">ChatLog: Recording and Analyzing ChatGPT Across Time</span>
              <span class="author">
                <em>Shangqing Tu</em>*, Chunyang Li*, Jifan Yu, Xiaozhi Wang, Lei Hou, Juanzi Li
              </span>
  
              <span class="periodical">
  
      <em>arxiv-2023-04</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2304.14106.pdf">pdf</a>] [<a
                      href="https://github.com/THU-KEG/ChatLog">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. 
        We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our project at https://github.com/THU-KEG/ChatLog.</p>
  </span>
  
          </div>
          </li>
    </ol>
    <h3 class="year">2025</h3>
    <ol class="bibliography">
      <li>
        <div id="ACMMM-2025">

            <span class="title">LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models</span>
            <span class="author">


              <em>Shangqing Tu</em>*, Yucheng Wang*, Daniel Zhang-Li, Yushi Bai, Jifan Yu, Yuhao Wu, Lei Hou, Huiqin Liu, Zhiyuan Liu, Bin Xu, Juanzi Li

  </span>

            <span class="periodical">

    <em>ACM MM25 (Oral)</em>

  </span>


            <span class="links">

  [<a class="abstract">Abs</a>][<a
                    href="https://arxiv.org/pdf/2502.14834">pdf</a>] [<a
                    href="https://github.com/THU-KEG/LongWriter-V">code</a>]








</span>

            <!-- Hidden abstract block -->

            <span class="abstract hidden">
    <p>Existing Large Vision-Language Models (LVLMs) can process inputs with context lengths up to 128k visual and text tokens, yet they struggle to generate coherent outputs beyond 1,000 words. We find that the primary limitation is the absence of long output examples during supervised fine-tuning (SFT). To tackle this issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158 examples, each with multiple input images, an instruction, and corresponding outputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that maintain high-fidelity to the input images, we employ Direct Preference Optimization (DPO) to the SFT model. Given the high cost of collecting human feedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which breaks long outputs into segments and uses iterative corrections to form preference pairs with the original outputs. Additionally, we develop MMLongBench-Write, a benchmark featuring six tasks to evaluate the long-generation capabilities of VLMs. Our 7B parameter model, trained with LongWriter-V-22k and IterDPO, achieves impressive performance on this benchmark, outperforming larger proprietary models like GPT-4o. Code and data: https://github.com/THU-KEG/LongWriter-V
    </p>
</span>

        </div>
        </li>
        <li>
        <div id="KDD-2025">

            <span class="title">Knowledge-to-Jailbreak: Investigating Knowledge-driven Jailbreaking Attacks for Large Language Models</span>
            <span class="author">


              <em>Shangqing Tu</em>*, Zhuoran Pan*, Wenxuan Wang, Zhexin Zhang, Yuliang Sun, Jifan Yu, Hongning Wang, Lei Hou, Juanzi Li

  </span>

            <span class="periodical">

    <em>KDD 2025 research track</em>

  </span>


            <span class="links">

  [<a class="abstract">Abs</a>][<a
                    href="https://arxiv.org/pdf/2406.11682?">pdf</a>] [<a
                    href="https://github.com/THU-KEG/Knowledge-to-Jailbreak/">code</a>]








</span>

            <!-- Hidden abstract block -->

            <span class="abstract hidden">
    <p>Large language models (LLMs) have been increasingly applied to various domains, which triggers increasing concerns about LLMs' safety on specialized domains, e.g. medicine. Despite prior explorations on general jailbreaking attacks, there are two challenges for applying existing attacks on testing the domain-specific safety of LLMs: (1) Lack of professional knowledge-driven attacks,  (2) Insufficient coverage of domain knowledge.   To bridge this gap, we propose a new task, knowledge-to-jailbreak, which aims to generate jailbreaking attacks from domain knowledge, requiring both attack effectiveness and knowledge relevance. 
We collect a large-scale dataset with 12,974 knowledge-jailbreak pairs and fine-tune a large language model as jailbreak-generator, to produce domain knowledge-specific jailbreaks. Experiments on 13 domains and 8 target LLMs demonstrate the effectiveness of jailbreak-generator in generating jailbreaks that are both threatening to the target LLMs and relevant to the given knowledge. We also apply our method to an out-of-domain knowledge base, showing that jailbreak-generator can generate jailbreaks that are comparable in harmfulness to those crafted by human experts. Data and code are available at: https://github.com/THU-KEG/Knowledge-to-Jailbreak/.
    </p>
</span>

        </div>
        </li>
        <li>
          <div id="ACL-2025">
  
              <span class="title">LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks</span>
              <span class="author">
  
  
               Yushi Bai*,  <em>Shangqing Tu</em>*, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li
  
    </span>
  
              <span class="periodical">
  
      <em>ACL 2025 Main</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2412.15204">pdf</a>] [<a
                      href="https://longbench2.github.io/">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring \emph{deep understanding and reasoning} across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding.
To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7\% accuracy under a 15-minute time constraint.
Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1\% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7\%, surpassing the human baseline by 4\%. 
These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. </p>
  </span>
  
          </div>
          </li>
          <li>
          <div id="ACL-2025">
  
              <span class="title">Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis</span>
              <span class="author">
               Kejian Zhu*, <em>Shangqing Tu</em>*, Zhuoran Jin, Lei Hou, Juanzi Li, Jun Zhao
  
    </span>
  
              <span class="periodical">
  
      <em>ACL 2025 Main</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2506.04142">pdf</a>] [<a
                      href="https://github.com/GaryStack/Trustworthy-Evaluation">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient (ρ) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation </p>
  </span>
  
          </div>
          </li>
    </ol>



    <h3 class="year">2024</h3>
      <ol class="bibliography">
        <li>
          <div id="Reval-2024">
  
              <span class="title">R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models</span>
              <span class="author">
                <em>Shangqing Tu</em>*, Yuanchun Wang*, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, Juanzi Li
              </span>
  
              <span class="periodical">
  
      <em>KDD24 ADS</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2406.11681">pdf</a>] [<a
                      href="https://github.com/THU-KEG/R-Eval">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>Large language models have achieved remarkable success on general NLP tasks, but they may fall short for domain-specific problems. Recently, various Retrieval-Augmented Large Language Models (RALLMs) are proposed to address this shortcoming. However, existing evaluation tools only provide a few baselines and evaluate them on various domains without mining the depth of domain knowledge. In this paper, we address the challenges of evaluating RALLMs by introducing the R-Eval toolkit, a Python toolkit designed to streamline the evaluation of different RAG workflows in conjunction with LLMs. Our toolkit, which supports popular built-in RAG workflows and allows for the incorporation of customized testing data on the specific domain, is designed to be user-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs across three task levels and two representative domains, revealing significant variations in the effectiveness of RALLMs across different tasks and domains. Our analysis emphasizes the importance of considering both task and domain requirements when choosing a RAG workflow and LLM combination. We are committed to continuously maintaining our platform at https://github.com/THU-KEG/R-Eval to facilitate both the industry and the researchers.</p>
  </span>
  
          </div>
          </li>
        <li>
          <div id="waterbench-2023">
  
              <span class="title">WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models</span>
              <span class="author">
                <em>Shangqing Tu</em>*, Yuliang Sun*, Yushi Bai, Jifan Yu, Lei Hou, Juanzi Li
              </span>
  
              <span class="periodical">
  
      <em>ACL 2024 Main</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2311.07138.pdf">pdf</a>] [<a
                      href="https://github.com/THU-KEG/WaterBench">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For \textbf{benchmarking procedure}, to ensure an apples-to-apples comparison, we first adjust each watermarking method's hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For \textbf{task selection}, we diversify the input and output length to form a five-category taxonomy, covering 9 tasks. (3) For \textbf{evaluation metric}, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate 4 open-source watermarks on 2 LLMs under 2 watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at https://github.com/THU-KEG/WaterBench.</p>
  </span>
  
          </div>
          </li>

          <li>
            <div id="iclr-2024">

              <span class="title">KoLA: Carefully Benchmarking World Knowledge of Large Language Models</span>
              <span class="author">
  
                Jifan Yu*, Xiaozhi Wang*, <em>Shangqing Tu</em>*, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Chunyang Li, Zheyuan Zhang, Yushi Bai, Yantao Liu, Amy Xin, Nianyi Lin, Kaifeng Yun, Linlu Gong, Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai Li, Yong Guan, Kaisheng Zeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu, Yuan Yao, Ning Ding, Lei Hou, Zhiyuan Liu, Bin Xu, Jie Tang, Juanzi Li
            </span>
  
              <span class="periodical">
  
      <em>ICLR24</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2306.09296.pdf">pdf</a>] [<a
                      href="https://github.com/THU-KEG/KoLA">code</a>][<a
                      href="https://kola.xlore.cn/">website</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: 
        (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge hallucination. We evaluate 21 open-source and commercial LLMs and obtain some intriguing findings. 
        The KoLA dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn/ and will be continuously updated to provide references for developing LLMs and knowledge-related systems.
      </p>
  </span>
  
          </div>
          </li>
      </ol>
    <h3 class="year">2023</h3>
    <ol class="bibliography">
        <li>
        <div id="cikm-2023">

            <span class="title">LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts</span>
            <span class="author">


              <em>Shangqing Tu</em>*, Zheyuan Zhang*, Jifan Yu, Chunyang Li, Siyu Zhang, Zijun Yao, Lei Hou, Juanzi Li

  </span>

            <span class="periodical">

    <em>CIKM 2023</em>

  </span>


            <span class="links">

  [<a class="abstract">Abs</a>][<a
                    href="https://arxiv.org/pdf/2308.05935.pdf">pdf</a>] [<a
                    href="https://github.com/THU-KEG/VTA">code</a>]








</span>

            <!-- Hidden abstract block -->

            <span class="abstract hidden">
    <p>Teaching assistants have played essential roles in the long history of education. However, few MOOC platforms are providing human or virtual teaching assistants to support learning for massive online students due to the complexity of real-world online education scenarios and the lack of training data. In this paper, we present a virtual MOOC teaching assistant, LittleMu with minimum labeled training data, to provide question answering and chit-chat services. Consisting of two interactive modules of heterogeneous retrieval and language model prompting, LittleMu first integrates structural, semi- and unstructured knowledge sources to support accurate answers for a wide range of questions. Then, we design delicate demonstrations named "Chain of Teach" prompts to exploit the large-scale pre-trained model to handle complex uncollected questions. Except for question answering, we develop other educational services such as knowledge-grounded chit-chat. 
      We test the system's performance via both offline evaluation and online deployment. Since May 2020, our LittleMu system has served over 80,000 users with over 300,000 queries from over 500 courses on XuetangX MOOC platform, which continuously contributes to a more convenient and fair education. Our code, services, and dataset will be available at https://github.com/THU-KEG/VTA.
    </p>
</span>

        </div>
        </li>
        <li>
          <div id="sigir-2023">
  
              <span class="title">MoocRadar: A Fine-grained and Multi-aspect Knowledge Repository for Improving Cognitive Student Modeling in MOOCs</span>
              <span class="author">
  
  
                Jifan Yu, Mengying Lu, Qingyang Zhong, Zijun Yao, <em>Shangqing Tu</em>, Zhengshan Liao, Xiaoya Li, Manli Li, Lei Hou, Hai-Tao Zheng, Juanzi Li, Jie Tang
  
  
    </span>
  
              <span class="periodical">
  
      <em>SIGIR 2023</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2304.02205.pdf">pdf</a>] [<a
                      href="https://github.com/THU-KEG/MOOC-Radar">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>Student modeling, the task of inferring a student's learning characteristics through their interactions with coursework, is a fundamental issue in intelligent education. Although the recent attempts from knowledge tracing and cognitive diagnosis propose several promising directions for improving the usability and effectiveness of current models, the existing public datasets are still insufficient to meet the need for these potential solutions due to their ignorance of complete exercising contexts, fine-grained concepts, and cognitive labels. 
        In this paper, we present MoocRadar, a fine-grained, multi-aspect knowledge repository consisting of 2,513 exercise questions, 5,600 knowledge concepts, and over 12 million behavioral records. Specifically, we propose a framework to guarantee a high-quality and comprehensive annotation of fine-grained concepts and cognitive labels. The statistical and experimental results indicate that our dataset provides the basis for the future improvements of existing methods. Moreover, to support the convenient usage for researchers, we release a set of tools for data querying, model adaption, and even the extension of our repository, which are now available at this https://github.com/THU-KEG/MOOC-Radar</p>
  </span>
  
          </div>
          </li>
    </ol>


      <h3 class="year">2022</h3>
      <ol class="bibliography">
          <li>
          <div id="coling-2022">

              <span class="title">UPER: Boosting Multi-Document Summarization with an Unsupervised Prompt-based Extractor</span>
              <span class="author">


       <em>Shangqing Tu</em>, Jifan Yu, Fangwei Zhu,   Juanzi Li, Lei Hou and Jian-Yun Nie.



    </span>

              <span class="periodical">

      <em>COLING 2022 (<strong>oral</strong>)</em>

    </span>


              <span class="links">

    [<a class="abstract">Abs</a>][<a
                      href="https://aclanthology.org/2022.coling-1.550.pdf">pdf</a>] [<a
                      href="https://github.com/THU-KEG/UPER">code</a>]








  </span>

              <!-- Hidden abstract block -->

              <span class="abstract hidden">
      <p>Multi-Document Summarization (MDS) commonly employs the 2-stage extract-then-abstract paradigm, which first extracts a relatively short meta-document, then feeds it into the deep neural networks to generate an abstract. Previous work usually takes the ROUGE score as the label for training a scoring model to evaluate source documents.
          However, the trained scoring model is prone to under-fitting for low-resource settings, as it relies on the training data. To extract documents effectively, we construct prompting templates that invoke the underlying knowledge in Pre-trained Language Model (PLM) to calculate the document and keyword's perplexity, which can assess the document's semantic salience. Our unsupervised approach can be applied as a plug-in to boost other metrics for evaluating a document's salience, thus improving the subsequent abstract generation. We get positive results on 2 MDS datasets, 2 data settings, and 2 abstractive backbone models, showing our method's effectiveness.</p>
  </span>

          </div>
          </li>
      </ol>
<h3 class="year">2021</h3>
<ol class="bibliography">
  <li>

<div id="fangweizhu-acl-2021">

    <span class="title">TWAG: A Topic-guided Wikipedia Abstract Generator</span>
    <span class="author">


        Fangwei Zhu, <em>Shangqing Tu</em>, Jiaxin Shi, Juanzi Li, Lei Hou and Tong Cui.


      
    </span>

    <span class="periodical">
    
      <em>ACL 2021 Main</em>

    </span>


  <span class="links">
  
    [<a class="abstract">Abs</a>][<a
          href="https://arxiv.org/pdf/2106.15135.pdf">pdf</a>] [<a
          href="https://github.com/THU-KEG/TWAG">code</a>]
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->

  <span class="abstract hidden">
      <p>Wikipedia abstract generation aims to distill a Wikipedia abstract from web sources and has met significant success by adopting multi-document summarization techniques.However, previous works generally view the abstract as plain text, ignoring the fact that it is a description of a certain entity and can be decomposed into different topics.
In this paper, we propose a two-stage model TWAG that guides the abstract generation with topical information.
First, we detect the topic of each input paragraph with a classifier trained on existing Wikipedia articles to divide input documents into different topics.
Then, we predict the topic distribution of each abstract sentence, and decode the sentence from topic-aware representations with a Pointer-Generator network.
We evaluate our model on the WikiCatSum dataset, and the results show that TWAG outperforms various existing baselines and is capable of generating comprehensive abstracts.
Our code and dataset can be accessed at https://github.com/THU-KEG/TWAG</p>
  </span>

</div>
</li>
<li>

<div id="fleps-2021">

    <span class="title">Piezoelectric And Machine Learning-Based Technique For Classifying Force Levels And Locations Of
Multiple Force Touch Events.</span>
    <span class="author">
        Sizhe Zhang, <em>Shangqing Tu</em> , Zhipeng Sui, Shuo Gao

    </span>

    <span class="periodical">
    
      <em>IEEE FLEPS</em>
    
    
      2021
    
    </span>


  <span class="links">
  
    [<a class="abstract">Abs</a>][<a class="page-link"
                                     href="https://shangqingtu.github.io/assets/fleps_paper.pdf">pdf</a>] [<a
          href="https://github.com/ShangQingTu/sensor_classify">code</a>]
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->

  <span class="abstract hidden">
    <p>Current commercial force touch panels can merely detect a single force touch’s location and amplitude. However, in many applications, multiple force touch events can occur at the same time among different locations of the touch panel. To satisfy this need, in this article, a piezoelectric and machine learning-based technique is proposed. Here, the piezoelectric film-based touch panel is used to detect different force levels, while the machine learning algorithm is developed to interpret the locations and strengths of user applied multiple force touch events. High detection accuracy of 92.3% for location determination and 88.2% for force level recognition is achieved. </p>
  </span>

</div>
</li></ol>



  </article>





</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2021 Shangqing Tu.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.


  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://shangqingtu.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="https://shangqingtu.github.io/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://shangqingtu.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://shangqingtu.github.io/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
