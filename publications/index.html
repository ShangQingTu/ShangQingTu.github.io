<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Shangqing Tu | publications</title>
  <meta name="description" content="hello">

  <link rel="shortcut icon" href="https://shangqingtu.github.io/assets/img/favicon.ico">

  <link rel="stylesheet" href="https://shangqingtu.github.io/assets/css/main.css">
  <link rel="canonical" href="https://shangqingtu.github.io/publications/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        Shangqing Tu
    </span>


    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="https://shangqingtu.github.io/">about</a>

        <!-- Blog -->
        <!-- <a class="page-link" href="https://shangqingtu.github.io/blog/">blog</a> -->

        <!-- News -->
        <a class="page-link" href="https://shangqingtu.github.io/news/">news</a>

        <!-- Pubs -->
        <a class="page-link" href="https://shangqingtu.github.io/publications/">publications</a>

        <!-- Teaching -->
<!--        <a class="page-link" href="https://shangqingtu.github.io/teaching/">teaching</a>-->


        <!-- Pages -->
<!--







            <a class="page-link" href="https://shangqingtu.github.io/news/">news</a>



            <a class="page-link" href="https://shangqingtu.github.io/publications/">publications</a>



            <a class="page-link" href="https://shangqingtu.github.io/teaching/">teaching</a>





         -->

        <!-- CV link -->
        <a class="page-link" href="https://shangqingtu.github.io/assets/CV__Academia_.pdf">cv</a>

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <h5 class="post-description">Generated by jekyll-scholar.</h5>
  </header>

  <article class="post-content publications clearfix">
  
    <h3 class="year">Preprints</h3>
    <ol class="bibliography">

        <li>
          <div id="chatlog-2023">
  
              <span class="title">ChatLog: Recording and Analyzing ChatGPT Across Time</span>
              <span class="author">
                <em>Shangqing Tu</em>*, Chunyang Li*, Jifan Yu, Xiaozhi Wang, Lei Hou, Juanzi Li
              </span>
  
              <span class="periodical">
  
      <em>arxiv-2023-04</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2304.14106.pdf">pdf</a>] [<a
                      href="https://github.com/THU-KEG/ChatLog">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>While there are abundant researches about evaluating ChatGPT on natural language understanding and generation tasks, few studies have investigated how ChatGPT's behavior changes over time. In this paper, we collect a coarse-to-fine temporal dataset called ChatLog, consisting of two parts that update monthly and daily: ChatLog-Monthly is a dataset of 38,730 question-answer pairs collected every month including questions from both the reasoning and classification tasks. ChatLog-Daily, on the other hand, consists of ChatGPT's responses to 1000 identical questions for long-form generation every day. We conduct comprehensive automatic and human evaluation to provide the evidence for the existence of ChatGPT evolving patterns. We further analyze the unchanged characteristics of ChatGPT over time by extracting its knowledge and linguistic features. 
        We find some stable features to improve the robustness of a RoBERTa-based detector on new versions of ChatGPT. We will continuously maintain our project at https://github.com/THU-KEG/ChatLog.</p>
  </span>
  
          </div>
          </li>
    </ol>
    <h3 class="year">2024</h3>
      <ol class="bibliography">
        <li>
          <div id="Reval-2024">
  
              <span class="title">R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models</span>
              <span class="author">
                <em>Shangqing Tu</em>*, Yuanchun Wang*, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, Juanzi Li
              </span>
  
              <span class="periodical">
  
      <em>KDD24 ADS</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://shangqingtu.github.io/assets/r_eval.pdf">pdf</a>] [<a
                      href="https://github.com/THU-KEG/R-Eval">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>Large language models have achieved remarkable success on general NLP tasks, but they may fall short for domain-specific problems. Recently, various Retrieval-Augmented Large Language Models (RALLMs) are proposed to address this shortcoming. However, existing evaluation tools only provide a few baselines and evaluate them on various domains without mining the depth of domain knowledge. In this paper, we address the challenges of evaluating RALLMs by introducing the R-Eval toolkit, a Python toolkit designed to streamline the evaluation of different RAG workflows in conjunction with LLMs. Our toolkit, which supports popular built-in RAG workflows and allows for the incorporation of customized testing data on the specific domain, is designed to be user-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs across three task levels and two representative domains, revealing significant variations in the effectiveness of RALLMs across different tasks and domains. Our analysis emphasizes the importance of considering both task and domain requirements when choosing a RAG workflow and LLM combination. We are committed to continuously maintaining our platform at https://github.com/THU-KEG/R-Eval to facilitate both the industry and the researchers.</p>
  </span>
  
          </div>
          </li>
        <li>
          <div id="waterbench-2023">
  
              <span class="title">WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models</span>
              <span class="author">
                <em>Shangqing Tu</em>*, Yuliang Sun*, Yushi Bai, Jifan Yu, Lei Hou, Juanzi Li
              </span>
  
              <span class="periodical">
  
      <em>ACL24 Main</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2311.07138.pdf">pdf</a>] [<a
                      href="https://github.com/THU-KEG/WaterBench">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For \textbf{benchmarking procedure}, to ensure an apples-to-apples comparison, we first adjust each watermarking method's hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For \textbf{task selection}, we diversify the input and output length to form a five-category taxonomy, covering 9 tasks. (3) For \textbf{evaluation metric}, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate 4 open-source watermarks on 2 LLMs under 2 watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at https://github.com/THU-KEG/WaterBench.</p>
  </span>
  
          </div>
          </li>

          <li>
            <div id="iclr-2024">

              <span class="title">KoLA: Carefully Benchmarking World Knowledge of Large Language Models</span>
              <span class="author">
  
                Jifan Yu*, Xiaozhi Wang*, <em>Shangqing Tu</em>*, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun Yao, Xiaohan Zhang, Hanming Li, Chunyang Li, Zheyuan Zhang, Yushi Bai, Yantao Liu, Amy Xin, Nianyi Lin, Kaifeng Yun, Linlu Gong, Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai Li, Yong Guan, Kaisheng Zeng, Ji Qi, Hailong Jin, Jinxin Liu, Yu Gu, Yuan Yao, Ning Ding, Lei Hou, Zhiyuan Liu, Bin Xu, Jie Tang, Juanzi Li
            </span>
  
              <span class="periodical">
  
      <em>ICLR24</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2306.09296.pdf">pdf</a>] [<a
                      href="https://github.com/THU-KEG/KoLA">code</a>][<a
                      href="https://kola.xlore.cn/">website</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>The unprecedented performance of large language models (LLMs) necessitates improvements in evaluations. Rather than merely exploring the breadth of LLM abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark (KoLA), in which we carefully design three crucial factors: 
        (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge hallucination. We evaluate 21 open-source and commercial LLMs and obtain some intriguing findings. 
        The KoLA dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn/ and will be continuously updated to provide references for developing LLMs and knowledge-related systems.
      </p>
  </span>
  
          </div>
          </li>
      </ol>
    <h3 class="year">2023</h3>
    <ol class="bibliography">
        <li>
        <div id="cikm-2023">

            <span class="title">LittleMu: Deploying an Online Virtual Teaching Assistant via Heterogeneous Sources Integration and Chain of Teach Prompts</span>
            <span class="author">


              <em>Shangqing Tu</em>*, Zheyuan Zhang*, Jifan Yu, Chunyang Li, Siyu Zhang, Zijun Yao, Lei Hou, Juanzi Li

  </span>

            <span class="periodical">

    <em>CIKM 2023</em>

  </span>


            <span class="links">

  [<a class="abstract">Abs</a>][<a
                    href="https://arxiv.org/pdf/2308.05935.pdf">pdf</a>] [<a
                    href="https://github.com/THU-KEG/VTA">code</a>]








</span>

            <!-- Hidden abstract block -->

            <span class="abstract hidden">
    <p>Teaching assistants have played essential roles in the long history of education. However, few MOOC platforms are providing human or virtual teaching assistants to support learning for massive online students due to the complexity of real-world online education scenarios and the lack of training data. In this paper, we present a virtual MOOC teaching assistant, LittleMu with minimum labeled training data, to provide question answering and chit-chat services. Consisting of two interactive modules of heterogeneous retrieval and language model prompting, LittleMu first integrates structural, semi- and unstructured knowledge sources to support accurate answers for a wide range of questions. Then, we design delicate demonstrations named "Chain of Teach" prompts to exploit the large-scale pre-trained model to handle complex uncollected questions. Except for question answering, we develop other educational services such as knowledge-grounded chit-chat. 
      We test the system's performance via both offline evaluation and online deployment. Since May 2020, our LittleMu system has served over 80,000 users with over 300,000 queries from over 500 courses on XuetangX MOOC platform, which continuously contributes to a more convenient and fair education. Our code, services, and dataset will be available at https://github.com/THU-KEG/VTA.
    </p>
</span>

        </div>
        </li>
        <li>
          <div id="sigir-2023">
  
              <span class="title">MoocRadar: A Fine-grained and Multi-aspect Knowledge Repository for Improving Cognitive Student Modeling in MOOCs</span>
              <span class="author">
  
  
                Jifan Yu, Mengying Lu, Qingyang Zhong, Zijun Yao, <em>Shangqing Tu</em>, Zhengshan Liao, Xiaoya Li, Manli Li, Lei Hou, Hai-Tao Zheng, Juanzi Li, Jie Tang
  
  
    </span>
  
              <span class="periodical">
  
      <em>SIGIR 2023</em>
  
    </span>
  
  
              <span class="links">
  
    [<a class="abstract">Abs</a>][<a
                      href="https://arxiv.org/pdf/2304.02205.pdf">pdf</a>] [<a
                      href="https://github.com/THU-KEG/MOOC-Radar">code</a>]
  
  
  
  
  
  
  
  
  </span>
  
              <!-- Hidden abstract block -->
  
              <span class="abstract hidden">
      <p>Student modeling, the task of inferring a student's learning characteristics through their interactions with coursework, is a fundamental issue in intelligent education. Although the recent attempts from knowledge tracing and cognitive diagnosis propose several promising directions for improving the usability and effectiveness of current models, the existing public datasets are still insufficient to meet the need for these potential solutions due to their ignorance of complete exercising contexts, fine-grained concepts, and cognitive labels. 
        In this paper, we present MoocRadar, a fine-grained, multi-aspect knowledge repository consisting of 2,513 exercise questions, 5,600 knowledge concepts, and over 12 million behavioral records. Specifically, we propose a framework to guarantee a high-quality and comprehensive annotation of fine-grained concepts and cognitive labels. The statistical and experimental results indicate that our dataset provides the basis for the future improvements of existing methods. Moreover, to support the convenient usage for researchers, we release a set of tools for data querying, model adaption, and even the extension of our repository, which are now available at this https://github.com/THU-KEG/MOOC-Radar</p>
  </span>
  
          </div>
          </li>
    </ol>


      <h3 class="year">2022</h3>
      <ol class="bibliography">
          <li>
          <div id="coling-2022">

              <span class="title">UPER: Boosting Multi-Document Summarization with an Unsupervised Prompt-based Extractor</span>
              <span class="author">


       <em>Shangqing Tu</em>, Jifan Yu, Fangwei Zhu,   Juanzi Li, Lei Hou and Jian-Yun Nie.



    </span>

              <span class="periodical">

      <em>COLING 2022 (<strong>oral</strong>)</em>

    </span>


              <span class="links">

    [<a class="abstract">Abs</a>][<a
                      href="https://aclanthology.org/2022.coling-1.550.pdf">pdf</a>] [<a
                      href="https://github.com/THU-KEG/UPER">code</a>]








  </span>

              <!-- Hidden abstract block -->

              <span class="abstract hidden">
      <p>Multi-Document Summarization (MDS) commonly employs the 2-stage extract-then-abstract paradigm, which first extracts a relatively short meta-document, then feeds it into the deep neural networks to generate an abstract. Previous work usually takes the ROUGE score as the label for training a scoring model to evaluate source documents.
          However, the trained scoring model is prone to under-fitting for low-resource settings, as it relies on the training data. To extract documents effectively, we construct prompting templates that invoke the underlying knowledge in Pre-trained Language Model (PLM) to calculate the document and keyword's perplexity, which can assess the document's semantic salience. Our unsupervised approach can be applied as a plug-in to boost other metrics for evaluating a document's salience, thus improving the subsequent abstract generation. We get positive results on 2 MDS datasets, 2 data settings, and 2 abstractive backbone models, showing our method's effectiveness.</p>
  </span>

          </div>
          </li>
      </ol>
<h3 class="year">2021</h3>
<ol class="bibliography">
  <li>

<div id="fangweizhu-acl-2021">

    <span class="title">TWAG: A Topic-guided Wikipedia Abstract Generator</span>
    <span class="author">


        Fangwei Zhu, <em>Shangqing Tu</em>, Jiaxin Shi, Juanzi Li, Lei Hou and Tong Cui.


      
    </span>

    <span class="periodical">
    
      <em>ACL 2021</em>

    </span>


  <span class="links">
  
    [<a class="abstract">Abs</a>][<a
          href="https://arxiv.org/pdf/2106.15135.pdf">pdf</a>] [<a
          href="https://github.com/THU-KEG/TWAG">code</a>]
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->

  <span class="abstract hidden">
      <p>Wikipedia abstract generation aims to distill a Wikipedia abstract from web sources and has met significant success by adopting multi-document summarization techniques.However, previous works generally view the abstract as plain text, ignoring the fact that it is a description of a certain entity and can be decomposed into different topics.
In this paper, we propose a two-stage model TWAG that guides the abstract generation with topical information.
First, we detect the topic of each input paragraph with a classifier trained on existing Wikipedia articles to divide input documents into different topics.
Then, we predict the topic distribution of each abstract sentence, and decode the sentence from topic-aware representations with a Pointer-Generator network.
We evaluate our model on the WikiCatSum dataset, and the results show that TWAG outperforms various existing baselines and is capable of generating comprehensive abstracts.
Our code and dataset can be accessed at https://github.com/THU-KEG/TWAG</p>
  </span>

</div>
</li>
<li>

<div id="fleps-2021">

    <span class="title">Piezoelectric And Machine Learning-Based Technique For Classifying Force Levels And Locations Of
Multiple Force Touch Events.</span>
    <span class="author">
        Sizhe Zhang, <em>Shangqing Tu</em> , Zhipeng Sui, Shuo Gao

    </span>

    <span class="periodical">
    
      <em>IEEE FLEPS</em>
    
    
      2021
    
    </span>


  <span class="links">
  
    [<a class="abstract">Abs</a>][<a class="page-link"
                                     href="https://shangqingtu.github.io/assets/fleps_paper.pdf">pdf</a>] [<a
          href="https://github.com/ShangQingTu/sensor_classify">code</a>]
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->

  <span class="abstract hidden">
    <p>Current commercial force touch panels can merely detect a single force touchâ€™s location and amplitude. However, in many applications, multiple force touch events can occur at the same time among different locations of the touch panel. To satisfy this need, in this article, a piezoelectric and machine learning-based technique is proposed. Here, the piezoelectric film-based touch panel is used to detect different force levels, while the machine learning algorithm is developed to interpret the locations and strengths of user applied multiple force touch events. High detection accuracy of 92.3% for location determination and 88.2% for force level recognition is achieved. </p>
  </span>

</div>
</li></ol>



  </article>





</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2021 Shangqing Tu.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.


  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="https://shangqingtu.github.io/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="https://shangqingtu.github.io/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="https://shangqingtu.github.io/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="https://shangqingtu.github.io/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
